
Advanced topics
===============
author: Bela Ban belaban@yahoo.com




Message flags
-------------
* Message flags are used to override default behavior provided by the protocols in a stack
* Examples:
** Mark a message as OOB (out-of-band) to deliver multiple messages from the same sender concurrently
** Skip the flow control protocols
* The advantage of tagging messages is that we don't need to change the configuration, but instead
  can override it on a per-message basis.

* API:

[source,java]
----
public Message setFlag(Flag ... flags);
----

* Example:

[source,java]
----
Message msg=new Message(null, "hello").setFlag(Message.Flag.OOB).name("A");
----


Provided message flags
----------------------
Message.OOB:: Marks a message as _out-of-band_. OOB messages don't not provide any ordering guarantees,
              although they're are reliable (no loss) and are delivered only once.

Message.DONT_BUNDLE:: Causes the transport not to bundle the message, but to send it immediately.

Message.NO_FC:: Bypasses any flow control protocols.

Message.NO_RELIABILITY:: Bypasses reliable protocols; makes the message unreliable. An unreliable message has no
                         ordering constraints, may get dropped or delivered multiple times.

Message.NO_TOTAL_ORDER:: If we use a total order configuration with `SEQUENCER`, then we
                         can bypass `SEQUENCER` (if we don't need total order for a given message) by tagging the message
                         with `NO_TOTAL_ORDER`.

Message.RSVP:: When this flag is set, a message send will block until the receiver (unicast) or receivers
               (multicast) have acked reception of the message, or until a timeout occurs.

Message.DONT_LOOPBACK:: When set, a message is multicast, but not looped back up the stack.
                        Useful when a sender doesn't want to receive its own multicast. +
                        Note that this is a _transient flag_, so `Message.isTransientFlagSet()` has
                        to be used instead of `Message.isFlagSet()`



OOB messages
------------
* Regular messages are ordered as follows
** Messages from the same sender are delivered FIFO: one-by-one
** Messages from different senders are delivered in parallel
*** When P sends P1 -> P2 -> P3, P1 is delivered first, then P2, then P3
*** P3 has to wait until both P1 and P2 have been processed
* However, _OOB messages_ completely ignore any ordering constraints the stack might have
** A message marked as OOB is processed by the OOB thread pool, not the regular thread pool (see below)
** When P sends P1 -> P2 -> P3, then all 3 messages are delivered randomly (but only once !)
*** Whether all 3 messages are really delivered in parallel also depends on the config of the OOB thread pool





The transport
-------------
* Bottom most protocol in the stack
* Serializes messages and sends them as UDP datagram packets, or sends them over a TCP connection
* Receives packets and de-serializes them into messages, which are then passed up the stack
* 4 thread pools for handling of incoming messages
. Regular thread pool
. OOB thread pool
. Internal thread pool
. Timer thread pool
* All thread pools can be configured (e.g. min/max threads)
* The thread pools can be replaced with custom thread pools
* The thread factories can also be replaced
* The transport also manages all sockets (UDP or TCP)


The transport
-------------
image::../images/Transport.png[The transport,width="60%",align=left,valign=top]
* UDP: 1 receiver thread for unicast datagram packets, 1 thread for multicast packets
* TCP: 1 thread per connection model; ie. in a 100 node cluster, we have 99 connection receiver threads in a node
** In 4.0, NIO.2 will be used to manage all TCP connections with a configurable pool of threads
* A receiver thread receives a network packet and - depending on the type - passes it to the right thread pool
** The pool performs
*** Version checking (drops packets with different version)
*** Deserializes the buffer into a `Message`
*** Passes the message (or message batch) up through the protocol stack all the way to the channel


Thread pools
------------
Regular:: Handles regular messages (non-OOB, non-internal)

OOB:: Handles OOB messages

Internal:: Handles `INTERNAL` messages. Reserved for use by JGroups. Needed to deliver some important messages by
           JGroups protocols (e.g. heartbeats in failure detection), without potentially blocking on user messages.

Timer:: Used to execute tasks (periodic or one-time), e.g. retransmission, expiry of connection pools, stability etc


Configuration of thread pools
-----------------------------
* All pools need to implement `java.util.concurrent.Executor` and the default implementations use
  `java.util.concurrent.ThreadPoolExecutor`
* The configuration of a thread pool is done with properties of the form <pool_name>.<attr>,
  e.g. `oob_thread_pool.min_threads`. The pool names are `thread_pool` (regular), `oob_thread_pool`, `internal` and
  `timer`
* The following attributes are used:

[width="90%",cols="2,10", frame="topbot",options="header"]
|====
| Name | Function
|enabled | If false, the thread pool is not enabled: when a message is received, it is passed up the stack by the receiver thread
| min_threads | The min number of threads
| max_threads | The max number of threads
| keep_alive_time | Time (ms) after which an idle thread should be returned to the pool
| queue_enabled | Whether or not a thread pool should have a queue enabled
| queue_max_size | The max size of a queue (if enabled)
| rejection_policy | The rejection policy. One of `run`, `discard`, `discardoldest` or `abort`
|====


Thread pool behavior
--------------------
* Semantics are the same as for `ThreadPoolExecutor`: on submission of a task:
* If we have fewer than `min_threads` -> create an additional thread
* Else:
** If a queue is enabled and not full -> queue the task
** Else create an additional thread if we're below `max_threads`
* If the queue is full (or disabled) or we've reached `max_threads` and all threads are busy -> consult the rejection policy

run:: Pass the message up on the receiver's thread. If there's a risk that the thread might block, or take a long time,
      this hampers the ability of the receiver to quickly remove packets and might thus lead to queue overflow
      (or a 0 TCP write window, _blocking the sender_)
discard:: Discard the message. JGroups retransmission will later retransmit the message, but dropping a message is
          good as it tells the sender to slow down a bit (via the flow control protocols)
abort:: same as discard, but throw an exception
discardoldest:: Discards the oldest message first


Thread pool use
---------------
* An OOB message uses 1 thread which passes it all the way up to the channel and to the application (see Transport)
** If the application sends another message (or invokes an RPC) _on the same thread_, that incoming thread can be
   busy for quite a while
* A regular message is passed up to the reliable protocol (either NAKACK for mcasts or UNICAST for unicasts)
** The message is then added to a table
** If there's no other thread busy removing messages from the table -> Set a CAS and
   remove as many messages as possible and pass them up
** Else return (thread is ready to process other messages)
** Most threads will only add their message (or message batch) to the table and return





Recommended configs
-------------------

[width="90%",cols="2,10",frame="topbot",options="header"]
|====
| Pool | Recommendation
| OOB | No queue -> OOB messages are executed on a thread or dropped. +
Set `min_size` to the number of cluster nodes, `max_size` should be higher than the max number of OOB messages received
at any given time
|Regular| Queue is enabled and quite large, to handle message peaks. +
Set `min_size` to the number of cluster nodes plus a few more (D), `max_size` to a slightly higher value. +
In a cluster of N, we never have more than N threads passing messages up, so we need D additional threads to
handle all other regular messages and add them to the retransmission tables. This is quick so D can be small.
|Internal| Leave the default config (min=2,max=4), don't touch
| Timer | Defaults (min=2,max=4,small queue) should be sufficient
|====



Message batching
----------------
* TBD



Asynchronous invocation API
---------------------------
* A method invoked in an RpcDispatcher is dispatched to application code
  by calling method handle from RequestHandler:

[source,java]
----
public interface RequestHandler {
    Object handle(Message msg) throws Exception;
}
----

* In the case of RpcDispatcher, the `handle()` method converts the message's contents into a method call,
  invokes the method against the target object and returns the result (or throws an exception). The return value
  of `handle()` is then sent back to the sender of the message.
        
* The invocation is _synchronous_, ie. done on the thread responsible for dispatching this
  particular message from the network up the stack all the way into the application. The thread is therefore
  _unusable_ for the duration of the method invocation.
        
* If the invocation takes a while, e.g. because locks are acquired or the application waits on some I/O, as
  the current thread is busy, another thread will be used for a different request message. This can quickly
  lead to the thread pool being exhausted or many messages getting queued if the pool has an associated queue.
        
* Therefore a new way of dispatching messages to the application was devised; the asynchronous invocation API:

[source,java]
----
public interface AsyncRequestHandler extends RequestHandler {
    void handle(Message request, Response response) throws Exception;
}
----

* Extending `RequestHandler`, `AsyncRequestHandler` adds an additional method taking a request message
  and a `Response` object. The request message contains the same information as before (e.g. a method call plus
  args). The `Response` argument is used to send a reply (if needed) at a later time, when processing is done.
        
[source,java]
----
public interface Response {
    void send(Object reply, boolean is_exception);
}    
----

* Response encapsulates information about the request (e.g. request ID and sender), and has method `reply()` to
  send a response. The `is_exception` parameter can be set to true if the reply is actually an exception, e.g.
  that was thrown when `handle()` ran application code.
        
* The advantage of the new API is that it can, but doesn't have to, be used asynchronously. The default
  implementation still uses the synchronous invocation style:

[source,java]
----
public void handle(Message request, Response response) throws Exception {
    Object retval=handle(request);
    if(response != null)
        response.send(retval, false);
}
----

* Method `handle()` is called, which synchronously calls into application code and returns a result, which is
  subsequently sent back to the sender of the request message.

* However, an application could subclass RpcDispatcher (as done in Infinispan), or it
could set a custom request handler via `setRequestHandler()`, and implement `handle()` by
dispatching the processing to a thread from a thread pool. The thread which guided the request message from
the network up to this point would be therefore immediately released and could be used for other messages.
The response would be sent whenever the invocation of application code is done, and thus the thread from
the thread pool would not be blocked on I/O, trying to acquire locks or anything else that blocks in
application code.
        
* To set the mode which is used, method `asyncDispatching(boolean)` can be used. This can be
  changed even at runtime, to switch between sync and async invocation style.

* Asynchrounous invocation is typically used in conjunction with an application thread pool. The application
  knows (JGroups doesn't) which requests can be processed in parallel and which ones can't. For example,
  all OOB calls could be dispatched directly to the thread pool, as ordering of OOB requests is not important,
  but regular requests should be added to a queue where they are processed sequentually.
        
* The main benefit here is that request dispatching (and ordering) is now under application control
  _if the application wants to do that_. If not, we can still use synchronous invocation.

* A good example where asynchronous invocation makes sense are replicated web sessions. If a cluster node A
  has 1000 web sessions, then replication of updates across the cluster generates messages from A. Because
  JGroups delivers messages from the _same_ sender _sequentially_, even
  updates to unrelated web sessions are delivered in strict order.

* With asynchronous invocation, the application could devise a dispatching strategy which assigns updates to
  different (unrelated) web sessions to any available thread from the pool, but queues updates to the same
  session, and processes those by the same thread, to provide ordering of updates to the same session. This
  would speed up overall processing, as updates to a web session 1 on A don't have to wait until all
  updates to an unrelated web session 2 on A have been processed.
