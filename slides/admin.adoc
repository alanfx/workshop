
Admin
=====
:author: Bela Ban belaban@yahoo.com
:backend: deckjs
:deckjs_transition: fade
:navigation:
:deckjs_theme: web-2.0
:goto:
:menu:
:toc:
:status:




Multicast routing
-----------------


Default stacks shipped
----------------------


Most frequent config changes
----------------------------


Switching from UDP to TCP
-------------------------


Tuning
------



Top JGroups problems
--------------------
* Mailing lists
* Support cases
* Consulting
* Interaction with customers
* Bug reports





Problem #12: AWS
---------------
Large packets sizes in EC2 are dropped::
The problem was that large packets using the default stack configuration for `FRAG2` (60k) were sometimes being dropped
between some hosts. The cluster would work fine until a large amount of data was sent between some pairs of servers. +
Amazon support: this is an update for case 85983221. We are currently limited to packet sizes of 32k and below on Amazon
EC2 and can confirm the issues you are facing for larger packet sizes. We are investigating a solution
to this limitation. Please let us know if you can keep your packet sizes below this level, or if this
is severe problem blocking your ability to operate. +
Solution: use `FRAG2` sizes of <= 32k if you are running in `UDP` mode under EC2.


Problem #11: last message dropped in `NAKACK2`
---------------------------------------------
Last message dropped issue::
Use `RSVP` to ack a batch of work, or set `resend_last_seqno` in `NAKACK2`




Problem #10: `receive()` and `viewAccepted()` callbacks
------------------------------------------------------
Invoking blocking RPCs or doing something long or blocking in these callbacks::
Because JGroups calls these callbacks on a thread from the incoming thread pool, all messages behind this one are stuck
until the callback returns +
Solution: use a separate thread is some callback code needs to block, invoke a blocking RPC, or perform a long task



Problem #9: MacOS
-----------------

Multicast routing on Mac OS::
https://developer.jboss.org/wiki/MulticastRoutingOnMacOSX +
Pick the correct `mcast_addr` in `UDP` based on the routing table and `bind_addr`



Problem #8: JGroups eating memory: NAKACK2 / STABLE
---------------------------------------------------
Memory grows in `NAKACK`::
In most cases, this is caused by a slow member which hasn't yet been suspected and excluded (hinders progress) +
Symptom: one or more slow members prevent an agreement between all members on which messages have been seen and can
be discarded -> memory accumulates +
Solution: remove / fix the slow or unresponsive members or decrease the failure detection timeout to exclude the member




Problem #7: seeing traffic from different clusters
--------------------------------------------------
When using UDP, we get warnings that traffic from a different cluster was discarded::
This is caused by using the same `mcast_addr` and `mcast_port` in `UDP` in different clusters +
Solution: use different values for either or both attributes in `UDP` for each separate cluster




Problem #6: TCPPING
-------------------
TCPPING.initial_hosts doesn't list all cluster members::
If `initial_hosts=A` and we have `{A,B,C}`, then `A` leaves, no new members can join +
Solution: list all members, use `send_cache_on_join` (`3.6.1` and higher) or use `MPING` (if IP multicasting is enabled)

TCPPING not merging::
Same as above: if we have `initial_hosts=A`, but 2 partitions `{A,B,C}` and `{X,Y,Z}`, then `X` will be able to send a
message to `A`, but `A` won't be able to respond (it doesn't have `X`'s address) -> no merge


TCPPING.initial_hosts lists the wrong members::
All members need to be listed with the `bind_addr` they're bound to and the `bind_port` they use

TCPPING is used but TCP doesn't set `bind_port`::
If `TCP.bind_port` is 0, a random port will be used and we cannot list it in `TCPPING.initial_hosts`



Problem #5: IPv6
----------------
Running in IPv6 without a correctly configured IPv6 routing table::
By default, the JDKs use IPv6, but the routing table is not configured correctly, or the config uses IPv4 +
Solution: look at IPv6 routing or force use of IPv4 (`-Djava.net.preferIPv4Stack=true`)

Mixing IPv4 and IPv6::
This works with TCP as IPv4 addresses are mapped to IPv4-mapped IPv6 addresses, but this is (IMO) hard to set up correctly

Wiki: https://developer.jboss.org/wiki/IPv6



Problem #4: JGroups versions
----------------------------

An old JGroups version is used::
Symptom: a bug that was fixed a long time ago pops up +
Side effect: Bela gets very tired having to waste time on some bug that's alread been fixed +
Solution: upgrade to the latest stable JGroups version

Different JGroups version in the same cluster::
Running different JGroups versions on different nodes might lead to subtle issues, e.g. dropping messages due to
deserialization issues. +
Solution: run the same version on all cluster nodes

Old JGroups configuration::
Sometimes, people upgrade to a newer JGroups version, but forget to upgrade their config(s) as well. +
Solution: always use the config template from the JGroups version you upgrade to and apply your specific changes



Problem #3: tinkering with configuration
----------------------------------------
(The "I'm smarter than Bela" problem)

Custom configuration files::
A configuration should never be built from the ground up; instead, copy `udp.xml` or `tcp.xml` from the JGroups JAR
and modify it

Removing \'unneeded' protocols::
Removing `UNICAST` because the transport is `TCP` (reliable): this won't work as `UNICAST` also performs ordering +
Symptoms: unicast messages can be unordered +
Removing `STABLE` causes OOMEs

Putting protocols in the wrong place::
A configuration needs to be defined in a certain order; placing protocols in the wrong place almost always causes subtle issues



Problem #2: cluster falls apart
-------------------------------
Low timeout in `FD` / `FD_ALL`::
GC, high network traffic or exhausted thread pools on the receivers can lead to missing heartbeats, causing members
to be suspected. +
Symptoms: some members are suspected, excluded and later merged back +
Solution: use high timeouts in heartbeat based failure detection protocols and add `FD_SOCK` / `FD_HOST`

IGMP Snooping::
Snooping (in the switch) listens on ports for IGMP joins and copies multicast packets for a groups to all joiners of
that group. Buggy code leads to that information getting dropped and multicast packets getting dropped until the
information has been refreshed. +
Symptoms: multicast groups falls apart every N minutes +
Solution: upgrade switch firmware



Problem #1: members don't find each other
-----------------------------------------
Binding to the loopback interface::
Setting `bind_addr` (in the transport) or system property `jgroups.bind_addr`
to `127.0.0.1` works when members are running on the same host, but doesn't work across hosts

Binding to the wrong network interface::
Binding to a VPN tunnel that's down, or `A` binding to `eth0` and `B` binding to `eth1` (different networks)

Firewalls dropping packets::
Disable the firewall, to see if this helps (e.g. `sudo iptables -F` on Linux). If this is the issue, open ports for
JGroups (`UDP.bind_port`, `FD_SOCK`, `STATE_SOCK`) and re-enable the firewall

Switch dropping packets::
Especially between VLANs. Check the swithc configuration

UDP: time-to-live loo low::
If `UDP` is used, increase the value of `ip_ttl`. See whether packets are received with wireshark / tcpdump




Logging
-------
* JGroups has no runtime dependencies on any logging framework (j.u.l. is used by default)
* At startup, JGroups looks for log4j2, log4j, j.u.l. (in this order)
** To force use of JDK logging, even if the log4j(2) JARs are present, `-Djgroups.use.jdk_logger=true` can be used
* Custom loggers can be used instead of the ones supported by default. To do this, interface
`CustomLogFactory` has to be implemented:

[source,java]
----
public interface CustomLogFactory {
    Log getLog(Class clazz);
    Log getLog(String category);
}
----

* The implementation needs to return an implementation of `org.jgroups.logging.Log`.
* To force using the custom log implementation, the fully qualified classname of the custom log
  factory has to be provided with `-Djgroups.logging.log_factory_class=com.foo.MyCustomLogger`.
* Ref: http://www.jgroups.org/manual/index.html#Logging


JMX
---
* JGroups exposes attributes and operations of the channel and all protocols via JMX
* Has to be enabled with `-Dcom.sun.management.jmxremote` (or others, ie. remote JMX host:port etc)
* To expose a channel and its attributes via JMX:

[source,java]
----
public static void registerChannel(JChannel channel,String name) {
    JmxConfigurator.registerChannel(channel,
                                    Util.getMBeanServer(),
                                    (name != null? name : "jgroups"),
                                    channel.getClusterName(),
                                    true);
}

// Util.registerChannel((JChannel)channel, channel.getClusterName());

public static void unregisterChannel(Channe channel) {
    JmxConfigurator.unregisterChannel((JChannel)channel,
                                      Util.getMBeanServer(),
                                      channel.getClusterName(());
}
----
* Let's try this out with our ChatDemo


probe.sh
--------
* Probe is a simple program which sends IP multicasts to a given multicast group and port and prints all responses
* Functionality
** Read attributes
** Write attributes
** Invoke operations
** Insert new protocols, remove protocols
* Probe requests are simple strings that are parsed by cluster nodes
* Probe responses are strings, too
* To enable:

[source,xml]
----
<UDP enable_diagnostics="true"
     diagnostics_addr="xxx"
     diagnostics_port="xxx"
     ...
/>
----
* Let's run ChatDemo and explore the features of probe


Listing all cluster nodes
-------------------------

----
[mac] /Users/bela/workshop/bin$ ./probe.sh

-- sending probe on /224.0.75.75:7500

#1 (149 bytes):
local_addr=A [f91dce0b-a753-987d-9d18-a8e8d86950ee]
cluster=ChatCluster
view=[A|1] (2) [A, B]
physical_addr=127.0.0.1:52181
version=3.6.0.Final

#2 (149 bytes):
local_addr=B [9e413b1d-d2f7-eaac-cb67-8eb94b2ba352]
cluster=ChatCluster
view=[A|1] (2) [A, B]
physical_addr=127.0.0.1:58998
version=3.6.0.Final


2 responses (2 matches, 0 non matches)
[mac] /Users/bela/workshop/bin$
----


Reading attributes from a protocol
----------------------------------
* Reading the number of sent and received messages and bytes in `UDP`:

----
[mac] /Users/bela/workshop/bin$ ./probe.sh jmx=UDP.num_msgs,num_byt

#1 (246 bytes):
local_addr=A [f91dce0b-a753-987d-9d18-a8e8d86950ee]
cluster=ChatCluster
view=[A|1] (2) [A, B]
physical_addr=127.0.0.1:52181
jmx=UDP={num_msgs_received=36, num_msgs_sent=37, num_bytes_received=2325, num_bytes_sent=2470}

version=3.6.0.Final


#2 (246 bytes):
local_addr=B [9e413b1d-d2f7-eaac-cb67-8eb94b2ba352]
cluster=ChatCluster
view=[A|1] (2) [A, B]
physical_addr=127.0.0.1:58998
jmx=UDP={num_msgs_received=36, num_msgs_sent=36, num_bytes_received=2372, num_bytes_sent=2325}

version=3.6.0.Final

2 responses (2 matches, 0 non matches)
[mac] /Users/bela/workshop/bin$
----



Setting attributes
------------------
* Changing the log level of `NAKACK2` to `TRACE`:
----
./probe.sh jmx=NAKACK2.level=trace
----
* This allows an admin to change the log level temporarily, and reset it back to `WARN` later



Invoking an operation
---------------------
* Dump the retransmit tables in `NAKACK2`:

----
[mac] /Users/bela/workshop/bin$ ./probe.sh op=NAKACK2.printMessages

#1 (254 bytes):
local_addr=A [f91dce0b-a753-987d-9d18-a8e8d86950ee]
cluster=ChatCluster
view=[A|1] (2) [A, B]
physical_addr=127.0.0.1:52181
NAKACK2.printMessages=A:
B: [0 | 0 | 0] (0 elements, 0 missing)
A: [2 | 9 | 9] (0 elements, 0 missing)

#2 (254 bytes):
local_addr=B [9e413b1d-d2f7-eaac-cb67-8eb94b2ba352]
cluster=ChatCluster
view=[A|1] (2) [A, B]
physical_addr=127.0.0.1:58998
NAKACK2.printMessages=B:
B: [0 | 0 | 0] (0 elements, 0 missing)
A: [9 | 9 | 9] (0 elements, 0 missing)

[mac] /Users/bela/workshop/bin$
----


Printing the protocol stacks
----------------------------

----
[mac] /Users/bela/workshop/bin$ ./probe.sh print-protocols

#1 (140 bytes):
protocols=UDP
PING
MERGE3
FD_SOCK
FD_ALL
NAKACK2
UNICAST3
STABLE
GMS
UFC
MFC
FRAG2

#2 (140 bytes):
protocols=UDP
PING
MERGE3
FD_SOCK
FD_ALL
NAKACK2
UNICAST3
STABLE
GMS
UFC
MFC
FRAG2

[mac] /Users/bela/workshop/bin$
----


Inserting a protocol at runtime
-------------------------------
* Insert `PRINT_BYTES` above `UDP`:
----
./probe.sh insert-protocol=org.lab.protocols.PRINT_BYTES=above=UDP
----
* Remove `PRINT_BYTES`:
----
./probe.sh remove-protocol=PRINT_BYTES
----
* Works only for stateless protocols
* Use cases
** Temporary TRACE logging to see what's going on in a defective system, then disable TRACE again
** Insert a protocol that extracts relevant information about a cluster, stores this to a file and sends the file to
   support



Use of probe when IP multicasting is not available
--------------------------------------------------
* `probe.sh -addr <address of any member> <diagnostics port (default: 7500)>`
** This asks any member for the addresses of _all members_ and then sends the probe request to all members in turn
* Note that any member can also be queried via simple datagram packets, e.g.:
----
[mac] /Users/bela/workshop/bin$ nc -u 192.168.1.3 7500
uuids
local_addr=A
uuids=2 elements:
B: ca335dc2-f30f-6e11-d13a-b029e3e9e2f1: 192.168.1.3:7801 (300 secs old)
A: 9dd407ae-577d-68b1-4f1e-6623279bb6ed: 192.168.1.3:7800 (31 secs old)

local_addr=A [9dd407ae-577d-68b1-4f1e-6623279bb6ed]
cluster=draw
view=[A|1] (2) [A, B]
physical_addr=192.168.1.3:7800
version=3.6.1.Final
^C
[mac] /Users/bela/workshop/bin$
----

* Ref: http://www.jgroups.org/manual/index.html#Probe

































